name: scrape
on:
  push:
    branches:
      - main
    paths:
      - get-videos.js
      - .github/workflows/scrape.yml
  schedule:
    # get the list of videos every night
    - cron: '0 4 * * *'

jobs:
  scrape:
    name: Scrape
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout üõé
        uses: actions/checkout@v3

      - name: Install dependencies üì¶
        uses: bahmutov/npm-install@v1

      # first, grab all videos from the YouTube playlist
      # and save them into "videos.json" overwriting it
      - name: Grab videos üì∫
        id: grab
        run: |
          node ./get-videos

          git status --porcelain videos.json

          function check() {
            if [[ -z "$(git status --porcelain videos.json)" ]];
            then
              echo "0"
            else
              echo "1"
            fi
          }

          echo ::set-output name=videos::$(check)
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}

      # the step "grab" sets the GitHub Actions output "videos"
      # that we can check to know if any of the videos have changed
      - name: Has changed videos?
        if: steps.grab.outputs.videos == 1
        run: echo "Have changed videos"

      # if there are new / changed videos,
      # recreate the entire Algolia index
      - name: Send to Algolia üîç
        if: steps.grab.outputs.videos == 1
        run: node ./upload-to-algolia
        env:
          APPLICATION_ID: ${{ secrets.ALGOLIA_APPLICATION_ID }}
          INDEX_NAME: videos
          ADMIN_API_KEY: ${{ secrets.ALGOLIA_ADMIN_API_KEY }}

      # and commit the changed videos.json file to repo
      - name: Commit any changed files üíæ
        uses: stefanzweifel/git-auto-commit-action@v4
        with:
          commit_message: Scraped videos
          branch: main
          file_pattern: videos.json
